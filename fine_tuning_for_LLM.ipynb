{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''extractive QA\n",
    "BERT - squad2.0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive/Projects/BERT QA'):\n",
    "    os.mkdir('/content/drive/MyDrive/Projects/BERT QA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-v2.0.json', 'rb') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Read SQuAD data from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - path: Path to the JSON file containing SQuAD data\n",
    "\n",
    "    Returns:\n",
    "    - contexts: List of contexts (passages)\n",
    "    - questions: List of questions\n",
    "    - answers: List of answers\n",
    "    \"\"\"\n",
    "    # Open the JSON file and load the data\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    # Initialize lists to store contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    # Iterate over groups in the SQuAD data\n",
    "    for group in squad.get('data', []):\n",
    "        # Iterate over paragraphs in the group\n",
    "        for passage in group.get('paragraphs', []):\n",
    "            # Get the context (passage)\n",
    "            context = passage.get('context', '')\n",
    "            # Iterate over questions and answers in the paragraph\n",
    "            for qa in passage.get('qas', []):\n",
    "                # Get the question\n",
    "                question = qa.get('question', '')\n",
    "                # Iterate over answers for the question\n",
    "                for answer in qa.get('answers', []):\n",
    "                    # Append context, question, and answer to their respective lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    # Return the lists of contexts, questions, and answers\n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Read training data\n",
    "train_contexts, train_questions, train_answers = read_data('train-v2.0.json')\n",
    "# Read validation data\n",
    "valid_contexts, valid_questions, valid_answers = read_data('dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_index(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # Check if the answer is correctly positioned\n",
    "        for offset in [0, -1, -2]:\n",
    "            if context[start_idx + offset:end_idx + offset] == gold_text:\n",
    "                # Update answer start and end indices\n",
    "                answer['answer_start'] = start_idx + offset\n",
    "                answer['answer_end'] = end_idx + offset\n",
    "                break  # Break loop once correct offset is found\n",
    "\n",
    "add_end_index(train_answers, train_contexts)\n",
    "add_end_index(valid_answers, valid_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    \"\"\"\n",
    "    Adds token positions for answers to encodings.\n",
    "\n",
    "    Parameters:\n",
    "    - encodings: Encodings object containing tokenized inputs\n",
    "    - answers: List of dictionaries containing answer positions\n",
    "\n",
    "    Returns:\n",
    "    None (modifies encodings in place)\n",
    "    \"\"\"\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Loop through each answer\n",
    "    for i, answer in enumerate(answers):\n",
    "        # Convert character positions to token positions\n",
    "        start_positions.append(encodings.char_to_token(i, answer['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answer['answer_end'] - 1))\n",
    "\n",
    "        # Handle cases where answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    # Update encodings with start and end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# Add token positions for training data\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "# Add token positions for validation data\n",
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for SQuAD.\n",
    "\n",
    "    Parameters:\n",
    "    - encodings: Encodings object containing tokenized inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - idx: Index of the item to retrieve\n",
    "\n",
    "        Returns:\n",
    "        Dictionary containing tensors for each key in the encodings\n",
    "        \"\"\"\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        Integer representing the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = SQuAD_Dataset(train_encodings)\n",
    "# Create validation dataset\n",
    "valid_dataset = SQuAD_Dataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the available device and use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Print the device being used\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for training: 3-9\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# Optimizer definition\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model.to(device)\n",
    "# Set model in training mode\n",
    "model.train()\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Create a progress bar for the training data\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    # Iterate over batches in the training data\n",
    "    for batch in loop:\n",
    "        # Zero gradients from previous iteration\n",
    "        optim.zero_grad()\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        # Compute the loss\n",
    "        loss = outputs[0]\n",
    "        # Backpropagation: compute gradients\n",
    "        loss.backward()\n",
    "        # Update model parameters\n",
    "        optim.step()\n",
    "\n",
    "        # Update progress bar description with current epoch\n",
    "        loop.set_description(f'Epoch {epoch+1}')\n",
    "        # Update progress bar with current loss\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Define the path where the model and tokenizer will be saved\n",
    "model_path = '/content/drive/MyDrive/Projects/BERT QA'\n",
    "\n",
    "# Save the model's weights, configuration, and vocabulary to the specified path\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "# Save the tokenizer's vocabulary and tokenizer configuration to the specified path\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where the pre-trained model and tokenizer are saved\n",
    "# model_path = '/content/drive/MyDrive/Projects/BERT QA'\n",
    "\n",
    "# # Load the pre-trained BERT model from the specified path\n",
    "# model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# # Load the tokenizer from the specified path\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# # Check the available device and use GPU if available, otherwise use CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# # Move the model to the appropriate device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Print the device being used\n",
    "# print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store accuracy values\n",
    "acc = []\n",
    "\n",
    "# Iterate over batches in the validation data\n",
    "for batch in tqdm(valid_loader):\n",
    "    with torch.no_grad():\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get predicted start and end positions\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # Compute accuracy for start positions and end positions\n",
    "        acc.append(((start_pred == start_true).sum() / len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum() / len(end_pred)).item())\n",
    "\n",
    "# Compute the average accuracy\n",
    "acc = sum(acc) / len(acc)\n",
    "\n",
    "# Print the header for true and predicted answer positions\n",
    "print(\"\\n\\nT/P\\tanswer_start\\tanswer_end\\n\")\n",
    "\n",
    "# Print true and predicted start and end positions for each example\n",
    "for i in range(len(start_true)):\n",
    "    print(f\"true\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
    "          f\"pred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(context, question):\n",
    "    \"\"\"\n",
    "    Get the predicted answer for a given context and question.\n",
    "\n",
    "    Parameters:\n",
    "    - context: The context in which the question is asked\n",
    "    - question: The question to be answered\n",
    "\n",
    "    Returns:\n",
    "    - answer: The predicted answer to the question\n",
    "    \"\"\"\n",
    "    # Tokenize the question and context\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted start and end positions\n",
    "    answer_start = torch.argmax(outputs[0])\n",
    "    answer_end = torch.argmax(outputs[1]) + 1\n",
    "\n",
    "    # Convert the predicted token IDs to string\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"\n",
    "    Normalize text by removing articles, punctuation, and standardizing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "    - s: Input text to be normalized\n",
    "\n",
    "    Returns:\n",
    "    - Normalized text\n",
    "    \"\"\"\n",
    "    import string, re\n",
    "\n",
    "    # Function to remove articles from text\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    # Function to fix white space in text\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    # Function to remove punctuation from text\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Function to convert text to lowercase\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    # Apply text normalization steps\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute exact match between predicted answer and true answer.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction: Predicted answer\n",
    "    - truth: True answer\n",
    "\n",
    "    Returns:\n",
    "    - Boolean indicating whether the prediction exactly matches the truth\n",
    "    \"\"\"\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute F1 score between predicted answer and true answer.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction: Predicted answer\n",
    "    - truth: True answer\n",
    "\n",
    "    Returns:\n",
    "    - F1 score\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # If either the prediction or the truth is no-answer then F1 score is 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # If there are no common tokens then F1 score is 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return round(2 * (prec * rec) / (prec + rec), 2)\n",
    "\n",
    "def question_answer(context, question, answer):\n",
    "    \"\"\"\n",
    "    Ask a question given a context and compare the predicted answer to the true answer.\n",
    "\n",
    "    Parameters:\n",
    "    - context: The context in which the question is asked\n",
    "    - question: The question to be answered\n",
    "    - answer: The true answer to the question\n",
    "\n",
    "    Returns:\n",
    "    None (prints the results)\n",
    "    \"\"\"\n",
    "    # Get the predicted answer for the question\n",
    "    prediction = get_prediction(context, question)\n",
    "    # Compute exact match score\n",
    "    em_score = exact_match(prediction, answer)\n",
    "    # Compute F1 score\n",
    "    f1_score = compute_f1(prediction, answer)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Prediction: {prediction}')\n",
    "    print(f'True Answer: {answer}')\n",
    "    print(f'Exact match: {em_score}')\n",
    "    print(f'F1 score: {f1_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).\"\"\"\n",
    "\n",
    "\n",
    "questions = [\"What did Albert Einstein develop?\",\n",
    "             \"Where was Albert Einstein born?\"]\n",
    "\n",
    "answers = [\"theory of relativity\", \"german\"]\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "  question_answer(context, question, answer)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
