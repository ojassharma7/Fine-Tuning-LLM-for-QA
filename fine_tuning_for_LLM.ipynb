{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''extractive QA\n",
    "BERT - squad2.0'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.exists('/content/drive/MyDrive/Projects/BERT QA'):\n",
    "    os.mkdir('/content/drive/MyDrive/Projects/BERT QA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import requests\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizerFast, BertForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train-v2.0.json', 'rb') as f:\n",
    "    squad = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    \"\"\"\n",
    "    Read SQuAD data from a JSON file.\n",
    "\n",
    "    Parameters:\n",
    "    - path: Path to the JSON file containing SQuAD data\n",
    "\n",
    "    Returns:\n",
    "    - contexts: List of contexts (passages)\n",
    "    - questions: List of questions\n",
    "    - answers: List of answers\n",
    "    \"\"\"\n",
    "    # Open the JSON file and load the data\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        squad = json.load(f)\n",
    "\n",
    "    # Initialize lists to store contexts, questions, and answers\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "\n",
    "    # Iterate over groups in the SQuAD data\n",
    "    for group in squad.get('data', []):\n",
    "        # Iterate over paragraphs in the group\n",
    "        for passage in group.get('paragraphs', []):\n",
    "            # Get the context (passage)\n",
    "            context = passage.get('context', '')\n",
    "            # Iterate over questions and answers in the paragraph\n",
    "            for qa in passage.get('qas', []):\n",
    "                # Get the question\n",
    "                question = qa.get('question', '')\n",
    "                # Iterate over answers for the question\n",
    "                for answer in qa.get('answers', []):\n",
    "                    # Append context, question, and answer to their respective lists\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "\n",
    "    # Return the lists of contexts, questions, and answers\n",
    "    return contexts, questions, answers\n",
    "\n",
    "# Read training data\n",
    "train_contexts, train_questions, train_answers = read_data('train-v2.0.json')\n",
    "# Read validation data\n",
    "valid_contexts, valid_questions, valid_answers = read_data('dev-v2.0.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_end_index(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        gold_text = answer['text']\n",
    "        start_idx = answer['answer_start']\n",
    "        end_idx = start_idx + len(gold_text)\n",
    "\n",
    "        # Check if the answer is correctly positioned\n",
    "        for offset in [0, -1, -2]:\n",
    "            if context[start_idx + offset:end_idx + offset] == gold_text:\n",
    "                # Update answer start and end indices\n",
    "                answer['answer_start'] = start_idx + offset\n",
    "                answer['answer_end'] = end_idx + offset\n",
    "                break  # Break loop once correct offset is found\n",
    "\n",
    "add_end_index(train_answers, train_contexts)\n",
    "add_end_index(valid_answers, valid_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "valid_encodings = tokenizer(valid_contexts, valid_questions, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    \"\"\"\n",
    "    Adds token positions for answers to encodings.\n",
    "\n",
    "    Parameters:\n",
    "    - encodings: Encodings object containing tokenized inputs\n",
    "    - answers: List of dictionaries containing answer positions\n",
    "\n",
    "    Returns:\n",
    "    None (modifies encodings in place)\n",
    "    \"\"\"\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Loop through each answer\n",
    "    for i, answer in enumerate(answers):\n",
    "        # Convert character positions to token positions\n",
    "        start_positions.append(encodings.char_to_token(i, answer['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answer['answer_end'] - 1))\n",
    "\n",
    "        # Handle cases where answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    # Update encodings with start and end positions\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "# Add token positions for training data\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "# Add token positions for validation data\n",
    "add_token_positions(valid_encodings, valid_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SQuAD_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for SQuAD.\n",
    "\n",
    "    Parameters:\n",
    "    - encodings: Encodings object containing tokenized inputs\n",
    "    \"\"\"\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Retrieves an item from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - idx: Index of the item to retrieve\n",
    "\n",
    "        Returns:\n",
    "        Dictionary containing tensors for each key in the encodings\n",
    "        \"\"\"\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the length of the dataset.\n",
    "\n",
    "        Returns:\n",
    "        Integer representing the length of the dataset\n",
    "        \"\"\"\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "# Create training dataset\n",
    "train_dataset = SQuAD_Dataset(train_encodings)\n",
    "# Create validation dataset\n",
    "valid_dataset = SQuAD_Dataset(valid_encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the available device and use GPU if available, otherwise use CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# Print the device being used\n",
    "print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs for training: 3-9\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# Optimizer definition\n",
    "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to the appropriate device (GPU if available, otherwise CPU)\n",
    "model.to(device)\n",
    "# Set model in training mode\n",
    "model.train()\n",
    "\n",
    "# Iterate over epochs\n",
    "for epoch in range(N_EPOCHS):\n",
    "    # Create a progress bar for the training data\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    # Iterate over batches in the training data\n",
    "    for batch in loop:\n",
    "        # Zero gradients from previous iteration\n",
    "        optim.zero_grad()\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "        # Compute the loss\n",
    "        loss = outputs[0]\n",
    "        # Backpropagation: compute gradients\n",
    "        loss.backward()\n",
    "        # Update model parameters\n",
    "        optim.step()\n",
    "\n",
    "        # Update progress bar description with current epoch\n",
    "        loop.set_description(f'Epoch {epoch+1}')\n",
    "        # Update progress bar with current loss\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# Define the path where the model and tokenizer will be saved\n",
    "model_path = '/content/drive/MyDrive/Projects/BERT QA'\n",
    "\n",
    "# Save the model's weights, configuration, and vocabulary to the specified path\n",
    "model.save_pretrained(model_path)\n",
    "\n",
    "# Save the tokenizer's vocabulary and tokenizer configuration to the specified path\n",
    "tokenizer.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the path where the pre-trained model and tokenizer are saved\n",
    "# model_path = '/content/drive/MyDrive/Projects/BERT QA'\n",
    "\n",
    "# # Load the pre-trained BERT model from the specified path\n",
    "# model = BertForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "# # Load the tokenizer from the specified path\n",
    "# tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# # Check the available device and use GPU if available, otherwise use CPU\n",
    "# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# # Move the model to the appropriate device\n",
    "# model = model.to(device)\n",
    "\n",
    "# # Print the device being used\n",
    "# print(f'Working on {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize a list to store accuracy values\n",
    "acc = []\n",
    "\n",
    "# Iterate over batches in the validation data\n",
    "for batch in tqdm(valid_loader):\n",
    "    with torch.no_grad():\n",
    "        # Move input tensors to the appropriate device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_true = batch['start_positions'].to(device)\n",
    "        end_true = batch['end_positions'].to(device)\n",
    "\n",
    "        # Forward pass through the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Get predicted start and end positions\n",
    "        start_pred = torch.argmax(outputs['start_logits'], dim=1)\n",
    "        end_pred = torch.argmax(outputs['end_logits'], dim=1)\n",
    "\n",
    "        # Compute accuracy for start positions and end positions\n",
    "        acc.append(((start_pred == start_true).sum() / len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_true).sum() / len(end_pred)).item())\n",
    "\n",
    "# Compute the average accuracy\n",
    "acc = sum(acc) / len(acc)\n",
    "\n",
    "# Print the header for true and predicted answer positions\n",
    "print(\"\\n\\nT/P\\tanswer_start\\tanswer_end\\n\")\n",
    "\n",
    "# Print true and predicted start and end positions for each example\n",
    "for i in range(len(start_true)):\n",
    "    print(f\"true\\t{start_true[i]}\\t{end_true[i]}\\n\"\n",
    "          f\"pred\\t{start_pred[i]}\\t{end_pred[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(context, question):\n",
    "    \"\"\"\n",
    "    Get the predicted answer for a given context and question.\n",
    "\n",
    "    Parameters:\n",
    "    - context: The context in which the question is asked\n",
    "    - question: The question to be answered\n",
    "\n",
    "    Returns:\n",
    "    - answer: The predicted answer to the question\n",
    "    \"\"\"\n",
    "    # Tokenize the question and context\n",
    "    inputs = tokenizer.encode_plus(question, context, return_tensors='pt').to(device)\n",
    "    # Perform inference using the model\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    # Get the predicted start and end positions\n",
    "    answer_start = torch.argmax(outputs[0])\n",
    "    answer_end = torch.argmax(outputs[1]) + 1\n",
    "\n",
    "    # Convert the predicted token IDs to string\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end]))\n",
    "\n",
    "    return answer\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"\n",
    "    Normalize text by removing articles, punctuation, and standardizing whitespace.\n",
    "\n",
    "    Parameters:\n",
    "    - s: Input text to be normalized\n",
    "\n",
    "    Returns:\n",
    "    - Normalized text\n",
    "    \"\"\"\n",
    "    import string, re\n",
    "\n",
    "    # Function to remove articles from text\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    # Function to fix white space in text\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    # Function to remove punctuation from text\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    # Function to convert text to lowercase\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    # Apply text normalization steps\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def exact_match(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute exact match between predicted answer and true answer.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction: Predicted answer\n",
    "    - truth: True answer\n",
    "\n",
    "    Returns:\n",
    "    - Boolean indicating whether the prediction exactly matches the truth\n",
    "    \"\"\"\n",
    "    return bool(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    \"\"\"\n",
    "    Compute F1 score between predicted answer and true answer.\n",
    "\n",
    "    Parameters:\n",
    "    - prediction: Predicted answer\n",
    "    - truth: True answer\n",
    "\n",
    "    Returns:\n",
    "    - F1 score\n",
    "    \"\"\"\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # If either the prediction or the truth is no-answer then F1 score is 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # If there are no common tokens then F1 score is 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return round(2 * (prec * rec) / (prec + rec), 2)\n",
    "\n",
    "def question_answer(context, question, answer):\n",
    "    \"\"\"\n",
    "    Ask a question given a context and compare the predicted answer to the true answer.\n",
    "\n",
    "    Parameters:\n",
    "    - context: The context in which the question is asked\n",
    "    - question: The question to be answered\n",
    "    - answer: The true answer to the question\n",
    "\n",
    "    Returns:\n",
    "    None (prints the results)\n",
    "    \"\"\"\n",
    "    # Get the predicted answer for the question\n",
    "    prediction = get_prediction(context, question)\n",
    "    # Compute exact match score\n",
    "    em_score = exact_match(prediction, answer)\n",
    "    # Compute F1 score\n",
    "    f1_score = compute_f1(prediction, answer)\n",
    "\n",
    "    # Print the results\n",
    "    print(f'Question: {question}')\n",
    "    print(f'Prediction: {prediction}')\n",
    "    print(f'True Answer: {answer}')\n",
    "    print(f'Exact match: {em_score}')\n",
    "    print(f'F1 score: {f1_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\"Albert Einstein was a German-born theoretical physicist who developed the theory of relativity, one of the two pillars of modern physics (alongside quantum mechanics).\"\"\"\n",
    "\n",
    "\n",
    "questions = [\"What did Albert Einstein develop?\",\n",
    "             \"Where was Albert Einstein born?\"]\n",
    "\n",
    "answers = [\"theory of relativity\", \"german\"]\n",
    "\n",
    "for question, answer in zip(questions, answers):\n",
    "  question_answer(context, question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# code version 1\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer, default_data_collator\n",
    "\n",
    "# Check if GPU is available, else use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load SQuAD dataset\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\") #change\n",
    "squad = squad.train_test_split(test_size=0.2)\n",
    "\n",
    "# Convert dataset to DataFrames\n",
    "df_train = pd.DataFrame.from_dict(squad[\"train\"].to_dict())\n",
    "df_test = pd.DataFrame.from_dict(squad[\"test\"].to_dict())\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def preprocess_data(data_frame):\n",
    "    \"\"\"\n",
    "    Preprocesses data for question answering model.\n",
    "\n",
    "    Args:\n",
    "        data_frame (pandas.DataFrame): DataFrame containing columns 'question', 'context', and 'answers'.\n",
    "\n",
    "    Returns:\n",
    "        datasets.Dataset: Preprocessed dataset containing input_ids, attention_mask,\n",
    "                          start_positions, and end_positions.\n",
    "    \"\"\"\n",
    "    questions = [q.strip() for q in data_frame[\"question\"]]\n",
    "    contexts = [c.strip() for c in data_frame[\"context\"]]\n",
    "    answers = data_frame['answers']\n",
    "\n",
    "    # Tokenize inputs\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        contexts,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mappings = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    # Determine start and end positions for answers\n",
    "    for i, offset_mapping in enumerate(offset_mappings):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = start_char + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        context_start = next(idx for idx, seq_id in enumerate(sequence_ids) if seq_id == 1)\n",
    "        context_end = next(idx for idx, seq_id in enumerate(sequence_ids[context_start:], start=context_start) if seq_id != 1) - 1\n",
    "\n",
    "        if offset_mapping[context_start][0] > end_char or offset_mapping[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(next(idx for idx, offset in enumerate(offset_mapping[context_start:], start=context_start) if offset[0] <= start_char) - 1)\n",
    "            end_positions.append(next(idx for idx, offset in enumerate(offset_mapping[context_end:], start=context_end) if offset[1] >= end_char) + 1)\n",
    "\n",
    "    data_frame[\"start_positions\"] = start_positions\n",
    "    data_frame[\"end_positions\"] = end_positions\n",
    "\n",
    "    # Create dataset\n",
    "    data = {\n",
    "        'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions': start_positions,\n",
    "        'end_positions': end_positions,\n",
    "    }\n",
    "    data_frame = pd.DataFrame(data)\n",
    "    dataset = Dataset.from_pandas(data_frame)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Preprocess train and test datasets\n",
    "train_dataset = preprocess_data(df_train)\n",
    "eval_dataset = preprocess_data(df_test)\n",
    "\n",
    "# Initialize model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\").to(device)\n",
    "\n",
    "# Initialize data collator\n",
    "data_collator = default_data_collator\n",
    "\n",
    "# Training arguments\n",
    "output_dir = \"./fine-tuned-model\"\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\",  # Disable logging\n",
    "    save_strategy=\"epoch\",  # Save checkpoint after each epoch\n",
    "    save_total_limit=3,  # Save only the last 3 checkpoints\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Evaluation function\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    start_logits, end_logits = predictions\n",
    "    start_positions, end_positions = labels\n",
    "\n",
    "    # Decode the predictions\n",
    "    start_logits = torch.from_numpy(start_logits)\n",
    "    end_logits = torch.from_numpy(end_logits)\n",
    "\n",
    "    predictions_start = torch.argmax(start_logits).item()\n",
    "    predictions_end = torch.argmax(end_logits).item()\n",
    "\n",
    "    exact_match = ((predictions_start == start_positions) & (predictions_end == end_positions)).sum().item()\n",
    "\n",
    "    return {\"exact_match\": exact_match}\n",
    "\n",
    "trainer.compute_metrics = compute_metrics\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model after training\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# Test user input\n",
    "# Load the fine-tuned model\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(output_dir)\n",
    "\n",
    "# Function to get answer from the model\n",
    "def get_answer(question, context):\n",
    "    inputs = tokenizer(\n",
    "        question,\n",
    "        context,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    answer_start = torch.argmax(answer_start_scores)\n",
    "    answer_end = torch.argmax(answer_end_scores) + 1\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0][answer_start:answer_end]))\n",
    "    return answer\n",
    "\n",
    "# Test the model interactively\n",
    "while True:\n",
    "    # Get user input\n",
    "    question = input(\"Enter a question (type 'quit' to exit): \")\n",
    "    if question.lower() == 'quit':\n",
    "        break\n",
    "    context = input(\"Enter the context: \")\n",
    "\n",
    "    # Get and print the answer\n",
    "    answer = get_answer(question, context)\n",
    "    print(\"Answer:\", answer)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
